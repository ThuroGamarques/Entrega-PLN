{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFouKxYX6g3-"
   },
   "source": [
    "# **Aula 04** - Extração de Características (Features) em Texto\n",
    "\n",
    "* Parte 1 - Corporas\n",
    "* Parte 2 - Extração de Características em Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_95q-CHRRcEP"
   },
   "source": [
    "## **Parte 1** - Trabalhando com Corporas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHGwl8rJSt7q"
   },
   "source": [
    "### Início do Processamento do Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jcG2ZpRSuY8"
   },
   "outputs": [],
   "source": [
    "def ler(nome_arquivo):\n",
    "  arquivo = open(nome_arquivo,'r', encoding='utf-8')\n",
    "  conteudo_arq = arquivo.read()\n",
    "  arquivo.close()\n",
    "  return conteudo_arq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1742604052621,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "CsH6LEE4SypU",
    "outputId": "439928b7-c946-428f-ae3a-9e568f362c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "219835\n"
     ]
    }
   ],
   "source": [
    "texto = ler('/content/drive/MyDrive/Colab Notebooks/2025.05 Ubirajara.txt')\n",
    "print(len(texto))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_0jmBxXTBst"
   },
   "source": [
    "### Primeira Etapa - Buscador de Palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8a7IIM5TCgL"
   },
   "outputs": [],
   "source": [
    "def buscador(alvo, texto):\n",
    "  texto = texto.replace('\\n',' ')\n",
    "  texto = texto.replace('\\t',' ')\n",
    "\n",
    "  ocorrencias = []\n",
    "\n",
    "  encontrado_aqui = texto.find(alvo, 0)\n",
    "\n",
    "  while encontrado_aqui > 0:\n",
    "    pos_inicial = encontrado_aqui - (40 - len(alvo))\n",
    "    trecho = texto[pos_inicial: pos_inicial + 80]\n",
    "\n",
    "    ocorrencias.append(trecho)\n",
    "\n",
    "    encontrado_aqui = texto.find(alvo, encontrado_aqui + 1)\n",
    "\n",
    "    return ocorrencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1742604105103,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "J9kPxeW-TGrh",
    "outputId": "16570bf1-94aa-473a-e781-dae0c9bce49f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i o chefe tocantim quem recebeu no peito a ponta farpada.  Quando o corpo robust\n"
     ]
    }
   ],
   "source": [
    "resultados = buscador(' peito', texto)\n",
    "\n",
    "for trecho in resultados:\n",
    "  print(trecho)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNKcGEO5TEcr"
   },
   "source": [
    "### Segunda Etapa - Limpeza dos Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "186883IUTKoa"
   },
   "outputs": [],
   "source": [
    "palavras = texto.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4_x4J5xxTOzY"
   },
   "outputs": [],
   "source": [
    "def limpar(lista):\n",
    "  lixo = '.,:;?!\"´`^~()[]{}\\/|@#$%¨&*-'\n",
    "  quase_limpo = [x.strip(lixo).lower() for x in lista]\n",
    "  return [x for x in quase_limpo if x.isalpha() or '-' not in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1742604146895,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "NteCZVs9TQTZ",
    "outputId": "41f0fc68-d35d-45ce-b7c0-04fbe3556ae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Corre-se', 'atrás', 'do', 'carro,', 'corre', 'se', 'o', 'carro', 'for', 'embora.']\n"
     ]
    }
   ],
   "source": [
    "teste = \"Corre-se atrás do carro, corre se o carro for embora.\"\n",
    "word = teste.split()\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1742604153197,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "TpYHUIMYTSv6",
    "outputId": "6c1b1807-8e6b-48ab-e349-c2a59f0b6475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['atrás', 'do', 'carro', 'corre', 'se', 'o', 'carro', 'for', 'embora']\n"
     ]
    }
   ],
   "source": [
    "print(limpar(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1742604159644,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "tqQv2TxpTUW2",
    "outputId": "1a160aaa-a202-4c2c-f52d-34ecbb11be54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37139\n",
      "36585\n"
     ]
    }
   ],
   "source": [
    "print(len(palavras))\n",
    "palavras = limpar(palavras)\n",
    "print(len(palavras))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W34MOGYT6l1j"
   },
   "source": [
    "## **Parte 2** - Extração de Características"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQLavAft6pnx"
   },
   "source": [
    "### Exemplo 1 - Implementando BOW\n",
    "\n",
    "Criar uma implementação simples de BoW usando CountVectorizer do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1742599449349,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "dRfeMcUqsXwC",
    "outputId": "36015daa-1983-4bb3-9593-c34b77f24c18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulario: {'gato': 3, 'cachorro': 1, 'brinca': 0, 'com': 2, 'rato': 4}\n",
      "Matriz BoW:\n",
      "[[0 1 0 1 0]\n",
      " [1 1 1 1 0]\n",
      " [0 0 0 1 1]\n",
      " [0 1 0 1 1]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# importando a ferramenta que irá criar a representação numérica\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# criando um corpus de documentos que será usado para criação do vocabulário\n",
    "documentos = [\n",
    "    \"gato e cachorro\",\n",
    "    \"gato brinca com cachorro\",\n",
    "    \"gato e rato\",\n",
    "    \"Cachorro, gato e rato\",\n",
    "    \"Gato\",\n",
    "    \"Cachorro e rato\"\n",
    "]\n",
    "# Criando um objeto para ser utilizado: transformar os documentos em vetores\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Criando a matriz de contagem\n",
    "X = vectorizer.fit_transform(documentos)\n",
    "  # fit >>> cria um vocabulário das palavras\n",
    "  # transforme >>> conta a frequencia de cada palavra no corpus\n",
    "\n",
    "# Imprimindo a Matriz e o Vocabulário gerado\n",
    "print(f\"Vocabulario: {vectorizer.vocabulary_}\")\n",
    "  # realiza o mapeamento do vocabulário para um índice da matriz\n",
    "\n",
    "print(\"Matriz BoW:\")\n",
    "print(X.toarray())\n",
    "  # mostra a frequencia de cada palavra dentro da matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_UOs81t6z9y"
   },
   "source": [
    "### Exemplo 2 - Implementando BOW junto com TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1742599460417,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "QVJS3MNdRnJR",
    "outputId": "5599cdbd-359b-400a-ae53-a63ecc1dcdc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulário BoW: {'gato': 3, 'cachorro': 1, 'brinca': 0, 'com': 2, 'rato': 4}\n",
      "Matriz BoW:\n",
      "[[0 1 0 1 0]\n",
      " [1 1 1 1 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 0 1 1]\n",
      " [0 2 0 1 0]\n",
      " [0 1 0 0 1]]\n",
      "\n",
      "Vocabulário TF-IDF: {'gato': 3, 'cachorro': 1, 'brinca': 0, 'com': 2, 'rato': 4}\n",
      "Matriz TF-IDF\n",
      "[[0.         0.65483184 0.         0.75577461 0.        ]\n",
      " [0.63763824 0.28304719 0.63763824 0.32667911 0.        ]\n",
      " [0.         0.45813442 0.         0.52875615 0.71451367]\n",
      " [0.         0.45813442 0.         0.52875615 0.71451367]\n",
      " [0.         0.8661285  0.         0.4998214  0.        ]\n",
      " [0.         0.53976033 0.         0.         0.84181874]]\n"
     ]
    }
   ],
   "source": [
    "# Importando as Bibliotecas\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "  # classe que transforma os documentos em vetores e realiza a contagem de frequencia\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "  # Classe que transforma os documentos em vetores e uma contagem de frequencia ponderada\n",
    "\n",
    "# Definindo o corpus\n",
    "documentos = [\n",
    "    \"gato e cachorro\",\n",
    "    \"gato brinca com cachorro\",\n",
    "    \"gato e rato, cachorro\",\n",
    "    \"Cachorro, gato e rato\",\n",
    "    \"Gato, cachorro, cachorro\",\n",
    "    \"Cachorro e rato\"\n",
    "]\n",
    "\n",
    "# Criando o modelo BoW\n",
    "vectorizer_bow = CountVectorizer()\n",
    "  # Instanciamento da classe em objeto para ser usado\n",
    "X_bow = vectorizer_bow.fit_transform(documentos)\n",
    "  # fit >>> realizar a transformação do vocabulário\n",
    "  # transform >> transforma cada vetor em um documento com a contagem de frequência\n",
    "\n",
    "# Imprimindo o Vocabulário e a Matriz\n",
    "print(f\"Vocabulário BoW: {vectorizer_bow.vocabulary_}\")\n",
    "print(\"Matriz BoW:\")\n",
    "print(X_bow.toarray())\n",
    "\n",
    "# Criando um modelo TF-IDF\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "  # realiza a instanciação da classe em objeto\n",
    "X_tfidf = vectorizer_tfidf.fit_transform(documentos)\n",
    "  # fit >>> realizar a transformação do vocabulário\n",
    "  # transform >> transforma cada vetor em um documento com a contagem de frequência ponderada\n",
    "\n",
    "# Imprimindo o Vocabulário e a Matriz TF-IDF\n",
    "print(f\"\\nVocabulário TF-IDF: {vectorizer_tfidf.vocabulary_}\")\n",
    "print(\"Matriz TF-IDF\")\n",
    "print(X_tfidf.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p0kbaZRL659v"
   },
   "source": [
    "### Exemplo 3 - Realizando o Pre processamento e a Extração de características do texto\n",
    "Construção de um exemplo de pré-processamento\n",
    "e representação de texto:\n",
    "* Limpeza de dados – removendo caracteres indesejados e normalizando o texto.\n",
    "* Tokenização – dividindo o texto em palavras individuais.\n",
    "* Remoção de stopwords – eliminando palavras comuns que não carregam significado semântico importante.\n",
    "* Lematização – reduzindo as palavras à sua forma base.\n",
    "* Representação de Texto – convertendo o texto processado em uma matriz numérica usando o modelo Bag of Words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "error",
     "timestamp": 1742604458382,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "ArGy0AUO69jq",
    "outputId": "7f98bbc5-dfe5-4d0c-db97-dff1687cf7a3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f247f95bdd49>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_lema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdocumentos_processados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocumentos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'documentos Pré-processados:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f247f95bdd49>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_lema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdocumentos_processados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpreprocessar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocumentos\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'documentos Pré-processados:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-f247f95bdd49>\u001b[0m in \u001b[0;36mpreprocessar_texto\u001b[0;34m(texto)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpreprocessar_texto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0mtexto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^a-zA-Zá-ÿ\\s]'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mtokens_tudo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpalavra\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens_tudo\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mtokens_lema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpalavra\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "documentos = [\n",
    "    \"Os cachorros são animais muito amigáveis e leais!\",\n",
    "    \"Eu gosto de gatos porque eles são independentes e fofos.\",\n",
    "    \"Cachorros e gatos podem ser ótimos animais de estimaçãos.\"\n",
    "]\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"portuguese\"))\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "  texto = re.sub(r'[^a-zA-Zá-ÿ\\s]','',texto)\n",
    "  tokens_tudo = word_tokenize(texto.lower())\n",
    "  tokens = [palavra for palavra in tokens_tudo if palavra not in stop_words]\n",
    "  tokens_lema = [lemmatizer.lemmatize(palavra) for palavra in tokens]\n",
    "  return ' '.join(tokens_lema)\n",
    "\n",
    "documentos_processados = [preprocessar_texto(doc) for doc in documentos]\n",
    "\n",
    "print('documentos Pré-processados:')\n",
    "for i, doc in enumerate(documentos_processados):\n",
    "  print(f\"Documento {i + 1}: {doc}\")\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "X_bow = vectorizer.fit_transform(documentos_processados)\n",
    "\n",
    "print(\"\\nVocabulário BoW:\", vectorizer.vocabulary_)\n",
    "print(\"Matriz BoW:\")\n",
    "print(X_bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fhs9gUoF7CJ0"
   },
   "source": [
    "### Exemplo 4 - WordEmbedding utilizando Word2Vex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "error",
     "timestamp": 1742603338597,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "YFxc7btQ7FJN",
    "outputId": "9cb701f8-090c-4d3f-89c5-3061438cb09f"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-549704c141b6>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \"\"\"\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Importação a Biblioteca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m   \u001b[0;31m# Word2Vec >>> criar os modelos de vetorização\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Algumas frases para aumentar o corpus:\n",
    "    [\"o\",\"papagaio\",\"está\",\"falando\"],\n",
    "    [\"a\",\"lhama\",\"vai\",\"cuspir\",\"em\",\"você\"],\n",
    "    [\"o\",\"cachorro\",\"gosta\",\"de\",\"bola\"],\n",
    "\"\"\"\n",
    "# Importação a Biblioteca\n",
    "from gensim.models import Word2Vec\n",
    "  # Word2Vec >>> criar os modelos de vetorização\n",
    "\n",
    "# criação do Corpus\n",
    "corpus = [\n",
    "    [\"o\",\"cachorro\",\"está\",\"dormindo\"],\n",
    "    [\"o\",\"gato\",\"está\",\"dormindo\"],\n",
    "    [\"o\",\"cachorro\",\"gosta\",\"de\",\"bola\"],\n",
    "    [\"cachorro\",\"gato\"],\n",
    "    [\"o\",\"papagaio\",\"está\",\"falando\"],\n",
    "    [\"a\",\"lhama\",\"vai\",\"cuspir\",\"em\",\"você\"]\n",
    "]\n",
    "\n",
    "# Criando o modelo de vetor\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=3, min_count=1,sg=1)\n",
    "  # sentences = define o texto a ser transformado em vetor\n",
    "  # vector_size = define a dimensão dos vetores que representarão as palavras\n",
    "  # window = define a janela de contexto. considera as palavras antes e as palavras depois no alvo para aprender\n",
    "  # min_count = ignora as palavras que aparecer 1 vez no corpus\n",
    "  # indica o modelo utilizado\n",
    "    # 1 = skip_gram;\n",
    "    # 0 = CBOW\n",
    "\n",
    "# obtem o vetor da palavra\n",
    "vector = model.wv['cachorro']\n",
    "\n",
    "# calcula a similaridade de duas palavras\n",
    "similarity = model.wv.similarity('cachorro','gato')\n",
    "  # similaridade cosseno, qt mais próximo de 1, maior similaridade\n",
    "\n",
    "print(\"Similaridade entre 'cachorro' e 'gato': \",similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383
    },
    "executionInfo": {
     "elapsed": 35,
     "status": "error",
     "timestamp": 1742599808048,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "y4ulyKro7H3J",
    "outputId": "fac1c0b1-b3cc-44ce-c2ce-854a191eff0d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-beceadd53d7c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m corpus = [\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cachorro\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"está\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"latindo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quintal\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m\"o\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gato\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"está\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"miando\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"no\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"telhado\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gensim'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "corpus = [\n",
    "    [\"o\", \"cachorro\", \"está\", \"latindo\", \"no\", \"quintal\"],\n",
    "    [\"o\", \"gato\", \"está\", \"miando\", \"no\", \"telhado\"],\n",
    "    [\"o\", \"pássaro\", \"está\", \"voando\", \"no\", \"céu\",'lua'],\n",
    "    [\"a\", \"bola\", \"está\", \"rolando\", \"no\", \"chão\"],\n",
    "    [\"a\", \"criança\", \"está\", \"brincando\", \"com\", \"o\", \"cachorro\"],\n",
    "    [\"o\", \"gato\", \"e\", \"o\", \"rato\", \"são\", \"inimigos\"],\n",
    "    [\"a\", \"água\", \"está\", \"quente\", \"na\", \"caneca\"],\n",
    "    [\"o\", \"sol\", \"está\", \"brilhando\", \"no\", \"céu\"],\n",
    "    [\"a\", \"lua\", \"está\", \"cheia\", \"hoje\",'no','céu'],\n",
    "    [\"a\", \"computador\", \"está\", \"ligado\", \"na\", \"mesa\"],\n",
    "    ['a','lua','esta','no','céu', 'lua','bonita']\n",
    "]\n",
    "\n",
    "model = Word2Vec(sentences=corpus, vector_size=100, window=5, min_count=1, sg=1)\n",
    "\n",
    "# Calculando a similaridade entre palavras\n",
    "print(f\"Similaridade entre cachorro e gato: {model.wv.similarity('cachorro', 'gato')}\")\n",
    "print(f\"Similaridade entre cachorro e bola: {model.wv.similarity('cachorro', 'bola')}\")\n",
    "print(f\"Similaridade entre céu e lua: {model.wv.similarity('céu', 'lua')}\")\n",
    "print(f\"Similaridade entre computador e mesa: {model.wv.similarity('computador', 'mesa')}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgkVr4XkHW1B2yAKpX2lBh",
   "collapsed_sections": [
    "WQLavAft6pnx",
    "G_UOs81t6z9y"
   ],
   "mount_file_id": "1T1W_6XktHcgWFPSmF5Ur1WP5x1X5fmIG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
