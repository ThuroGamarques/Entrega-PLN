{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwoufOqw35Vi"
   },
   "source": [
    "# **Aula 03** - Processamento de Texto e Pr√©-processamento de Dados\n",
    "\n",
    "O pr√©-processamento limpa e transforma esse texto para facilitar o trabalho do algoritmo, deixando s√≥ as informa√ß√µes relevantes. T√©cnicas de Pr√©-processamento de Texto:\n",
    "1. **Normaliza√ß√£o de Texto** - Ajusta do texto para ter uma grafia padronizada;\n",
    "2. **Remo√ß√£o de Ru√≠do** - Retirar elementos do texto que n√£o agregam valor √† an√°lise e podem atrapalhar;\n",
    "3. **Tokeniza√ß√£o** - Tokenizar √© dividir o texto em pequenas unidades;\n",
    "4. **Remo√ß√£o de Stopwords** - Remover palavras que n√£o carregam muito significado para an√°lise;\n",
    "5. **Stemming** - T√©cnica para reduzir palavras √†s suas ra√≠zes ou radicais, cortando sufixos e prefixos;\n",
    "6. **Lematiza√ß√£o** - Redu√ß√£o da palavra √† sua forma de dicion√°rio (forma can√¥nica)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehZIl9uq38bV"
   },
   "source": [
    "## 1. Normaliza√ß√£o de texto e Remo√ß√£o de Ru√≠do\n",
    "* Remover caracteres especiais, pontua√ß√µes, e normalizar o uso de letras mai√∫sculas e min√∫sculas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 47,
     "status": "ok",
     "timestamp": 1742597421297,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "pqcEcy5k3_CY",
    "outputId": "6d5c2237-913d-49cc-86c8-936c34da527c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\n",
      "\n",
      "Texto limpo: Ol√° Este √© um exemplo de texto com v√°rias PONTUA√á√ïES s√≠mbolos especiais e LETRAS mai√∫sculas\n",
      "\n",
      "Texto normalizado: ol√° este √© um exemplo de texto com v√°rias pontua√ß√µes s√≠mbolos especiais e letras mai√∫sculas\n"
     ]
    }
   ],
   "source": [
    " # importa a biblioteca para trabalhar com express√µes regulares\n",
    "import re\n",
    "\n",
    "original = \"Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\"\n",
    "\n",
    "texto_limpo = re.sub(r'[^A-Za-z√Ä-√ø\\s]', '',original)\n",
    "  # re.sub() fun√ß√£o que realiza substitui√ß√£o\n",
    "  # r'[^A-Za-z√Ä-√ø\\s]'>>> express√£o regular que define um conjunto de caracteres a serem removidos\n",
    "    # [A-Za-z√Ä-√ø\\s] >>> define um conjunto de caracteres de A at√© Z, a at√© z e acentos e espa√ßos\n",
    "    # ^faz a nega√ß√£o de uma express√£o regular\n",
    "  # '' substitui a express√£o regular por uma string vazia\n",
    "\n",
    "texto_normalizado = texto_limpo.lower()\n",
    "\n",
    "print(f'Texto original: {original}')\n",
    "print(f'\\nTexto limpo: {texto_limpo}')\n",
    "print(f'\\nTexto normalizado: {texto_normalizado}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "21kKHdWE4CUG"
   },
   "source": [
    "## 2. Tokeniza√ß√£o\n",
    "* Tokeniza√ß√£o √© dividir o texto em unidades menores (tokens), que geralmente s√£o palavras ou pontua√ß√µes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1742597477165,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "JHlAWVc54DBZ",
    "outputId": "839219b3-32c6-4ec2-864d-cc457719b135"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: Ol√°!!! Este √© um exemplo de texto, com v√°rias PONTUA√á√ïES, s√≠mbolos #especiais, e LETRAS mai√∫sculas.\n",
      "\n",
      "\n",
      "Texto limpo: Ol√° Este √© um exemplo de texto com v√°rias PONTUA√á√ïES s√≠mbolos especiais e LETRAS mai√∫sculas\n",
      "\n",
      "\n",
      "Texto normalizado: ol√° este √© um exemplo de texto com v√°rias pontua√ß√µes s√≠mbolos especiais e letras mai√∫sculas\n",
      "\n",
      "\n",
      "Tokens extraidos: ['ol√°', 'este', '√©', 'um', 'exemplo', 'de', 'texto', 'com', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'e', 'letras', 'mai√∫sculas']\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#nltk.download('punkt_tab')\n",
    "\n",
    "tokens = word_tokenize(texto_normalizado)\n",
    "\n",
    "print(f'Texto original: {original}')\n",
    "print(f'\\n\\nTexto limpo: {texto_limpo}')\n",
    "print(f'\\n\\nTexto normalizado: {texto_normalizado}')\n",
    "print(f'\\n\\nTokens extraidos: {tokens}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fipPXw24F_S"
   },
   "source": [
    "## 3. Remo√ß√£o de Stopwords\n",
    "* Stopwords s√£o palavras de pouco valor sem√¢ntico (como \"de\", \"a\", \"o\") que podem ser removidas para simplificar o texto\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1742597505683,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "U5rkt8g-4HVm",
    "outputId": "3b04e8c2-db4b-4fca-e75f-d179e39335ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'√†s', 'das', 'lhe', 'ser√°', 'ou', 's√≥', 'ela', 'num', 'nossas', 'sejamos', 'estiverem', 'estivermos', 'uma', 'haver', 'isto', 'estas', 'aqueles', 'por', 'pelos', 'n√≥s', 'tuas', 'nossa', 'estivera', 'estiv√©ssemos', 'h√£o', 'te', 'formos', 'estamos', 'houvera', 'em', 't√≠nhamos', 'tu', 'f√¥ssemos', 'pelas', 'ser√£o', 'tivesse', 'houv√©ssemos', 'esteve', 'tive', 'estiv√©ramos', 'tivemos', 'houveriam', 'era', 'tiv√©ramos', 'tiverem', 'tivera', 'vos', 'da', 'o', 'isso', 'voc√™s', 'seremos', 'fossem', '√©', 'meu', 'entre', 'aquele', 'fosse', '√©ramos', 'hajamos', 'tem', 'tenha', 'houvesse', 'eles', 'seja', 'hei', 'com', 'fora', 'nos', 'os', 'sem', 'dos', 'nossos', 'sou', 'aos', 'for', 'h√°', 'tiver', 'teus', 's√£o', 'minhas', 'me', 'est√°', 'est√°vamos', 'seriam', '√†', 'estive', 'esteja', 'mais', 'estes', 'tamb√©m', 'estou', 'teria', 'temos', 'ser√≠amos', 'como', 'aquela', 'depois', 'as', 'estavam', 'houve', 'esses', 'seus', 'teriam', 'estejamos', 'tiveram', 'que', 'f√¥ramos', 'houverem', 'estar', 'se', 'houvermos', 'fui', 'suas', 'j√°', 'estivessem', 'meus', 'houveria', 'lhes', 'ele', 'mesmo', 'nas', 'nem', 'dele', 'terei', 'muito', 'houver', 'mas', 'houverei', 'estiver', 'n√£o', 'quando', 'hajam', 'na', 'estejam', 'ser', 'somos', 'tinham', 'a', 'houvemos', 'pela', 'no', 'nosso', 'havemos', 'numa', 'haja', 't√©m', 'ter√°', 'tenhamos', 'aquelas', 'foi', 'fomos', 'pelo', 'de', 'este', 'houvessem', 'essa', 'seu', 'seria', 'estivesse', 'forem', 'minha', 'tivessem', 'elas', 'eu', 'qual', 'est√£o', 'houveremos', 'estava', 'at√©', 'teremos', 'teu', 'houver√°', 'houver√£o', 'tinha', 'um', 'houver√≠amos', 'deles', 'estivemos', 'tenho', 'tivermos', 'ao', 'eram', 'ter√≠amos', 'voc√™', 'sejam', 'teve', 'dela', 'tua', 'aquilo', 'estiveram', 'essas', 'esta', 'ter√£o', 'quem', 'delas', 'para', 'do', 'esse', 'foram', 'tenham', 'e', 'tiv√©ssemos', 'serei', 'houv√©ramos', 'houveram', 'sua'}\n",
      "\n",
      "\n",
      "Tokens extraidos: ['ol√°', 'este', '√©', 'um', 'exemplo', 'de', 'texto', 'com', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'e', 'letras', 'mai√∫sculas'] + \n",
      " quantidade de tokens: 15\n",
      "\n",
      "\n",
      "Tokens extraidos: ['ol√°', 'exemplo', 'texto', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'letras', 'mai√∫sculas'] + \n",
      " quantidade de tokens: 9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "\n",
    "print(stopwords_pt)\n",
    "\n",
    "tokens_sem_stopwords = [palavra for palavra in tokens if palavra.lower() not in stopwords_pt]\n",
    "\n",
    "print(f'\\n\\nTokens extraidos: {tokens} + \\n quantidade de tokens: {len(tokens)}')\n",
    "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords} + \\n quantidade de tokens: {len(tokens_sem_stopwords)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SlJ_vLBt4KV-"
   },
   "source": [
    "## 4. Stemming e Lemaliza√ß√£o\n",
    "\n",
    "\n",
    "*   Stemming reduz as palavras √†s suas ra√≠zes (ou radicais);\n",
    "*   Lematiza√ß√£o normaliza as palavras para suas formas base, levando em conta contexto e gram√°tica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1742597515465,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "oIBYMgxo4L4L",
    "outputId": "1960c7dd-0c1f-456d-ae29-3ca10431dea6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Tokens extraidos: ['ol√°', 'exemplo', 'texto', 'v√°rias', 'pontua√ß√µes', 's√≠mbolos', 'especiais', 'letras', 'mai√∫sculas']\n",
      "\n",
      "\n",
      "Tokens radicais: ['ol√°', 'exempl', 'text', 'v√°r', 'pontu', 's√≠mbol', 'espec', 'letr', 'mai√∫scul']\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping stemmers/rslp.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RSLPStemmer\n",
    "\n",
    "nltk.download('rslp')\n",
    "\n",
    "stemmer = RSLPStemmer()\n",
    "stemming = [stemmer.stem(palavra) for palavra in tokens_sem_stopwords]\n",
    "print(f'\\n\\nTokens extraidos: {tokens_sem_stopwords}')\n",
    "print(f'\\n\\nTokens radicais: {stemming}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qd4LujxV4Nyj"
   },
   "source": [
    "### Exemplo 01 - Pr√© Processamento completo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29337,
     "status": "ok",
     "timestamp": 1742597547965,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "PMlW8d6i4OnG",
    "outputId": "415ce5a8-c004-46ce-8a4c-1a9d49dbf84e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insira um texto que seja coerente, podendo ter s√≠mbolos: Aqui no meu computador, tem alguem me chamando toda hora. esta me deixando louco.\n",
      "['aqui', 'computador', 'alguem', 'chamando', 'toda', 'hora', 'deixando', 'louco']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "# Download dos recursos do NLTK (se necess√°rio)\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Texto de exemplo\n",
    "texto = input(\"Insira um texto que seja coerente, podendo ter s√≠mbolos: \")\n",
    "\n",
    "# Limpeza de ru√≠dos e normaliza√ß√£o\n",
    "texto_limpo = re.sub(r'[^a-zA-Z]', ' ', texto)  # Remove tudo que n√£o for letra e substitui por espa√ßo\n",
    "texto_lower = texto_limpo.lower()  # Converte para min√∫sculas\n",
    "\n",
    "# Tokeniza√ß√£o\n",
    "tokens = nltk.word_tokenize(texto_lower)\n",
    "\n",
    "# Remo√ß√£o de stopwords\n",
    "stop_words = set(stopwords.words('portuguese'))\n",
    "palavras_filtradas = [palavra for palavra in tokens if palavra not in stop_words]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "palavras_stemizadas = [stemmer.stem(palavra) for palavra in palavras_filtradas]\n",
    "\n",
    "# Impress√£o do resultado final\n",
    "print(palavras_stemizadas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5UZsvKgU4P_G"
   },
   "source": [
    "### Exemplo 02 - Estrutura de Pr√©-processamento de texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16499,
     "status": "ok",
     "timestamp": 1742597570987,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "a9mJilB_4RLq",
    "outputId": "29822986-7b46-4f39-a167-170431404563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Collecting pt-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-3.8.0/pt_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pt-core-news-sm\n",
      "Successfully installed pt-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('pt_core_news_sm')\n",
      "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download pt_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1630,
     "status": "ok",
     "timestamp": 1742597577472,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "XMgDhMt84SuC",
    "outputId": "026ed558-f426-4ece-811e-e4abbcbd188f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto Original:\n",
      " O Processamento de Linguagem Natural (PLN) √© uma √°rea essencial da intelig√™ncia artificial! üòä Confira mais em: https://exemplo.com\n",
      "\n",
      "Texto Normalizado:\n",
      " o processamento de linguagem natural pln √© uma √°rea essencial da intelig√™ncia artificial  confira mais em \n",
      "\n",
      "Tokens:\n",
      " ['o', 'processamento', 'de', 'linguagem', 'natural', 'pln', '√©', 'uma', '√°rea', 'essencial', 'da', 'intelig√™ncia', 'artificial', 'confira', 'mais', 'em']\n",
      "\n",
      "Tokens Sem Stopwords:\n",
      " ['processamento', 'linguagem', 'natural', 'pln', '√°rea', 'essencial', 'intelig√™ncia', 'artificial', 'confira']\n",
      "\n",
      "Stemming:\n",
      " ['process', 'lingu', 'natur', 'pln', '√°re', 'essenc', 'intelig', 'artific', 'conf']\n",
      "\n",
      "Lematiza√ß√£o:\n",
      " ['processamento', 'linguagem', 'natural', 'pln', '√°rea', 'essencial', 'intelig√™ncia', 'artificial', 'confira']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "\n",
    "# Baixar stopwords do NLTK (se necess√°rio)\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('rslp')\n",
    "\n",
    "# Carregar modelo do spaCy (portugu√™s como exemplo, pode trocar para 'en_core_web_sm' se for ingl√™s)\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Texto de exemplo (pode ser uma review ou trecho de not√≠cia)\n",
    "texto = \"O Processamento de Linguagem Natural (PLN) √© uma √°rea essencial da intelig√™ncia artificial! üòä Confira mais em: https://exemplo.com\"\n",
    "\n",
    "# 1. Normaliza√ß√£o (remover acentos, transformar em min√∫sculas, etc.)\n",
    "def normalizar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = re.sub(r'https?://\\S+|www\\.\\S+', '', texto)  # Remover URLs\n",
    "    texto = re.sub(r'[^a-zA-Z√°-√∫√Å-√ö√ß√á ]', '', texto)     # Remover caracteres especiais (ajuste para outros idiomas)\n",
    "    return texto\n",
    "\n",
    "texto_normalizado = normalizar_texto(texto)\n",
    "\n",
    "# 2. Tokeniza√ß√£o (nltk)\n",
    "tokens = nltk.word_tokenize(texto_normalizado, language='portuguese')\n",
    "\n",
    "# 3. Remo√ß√£o de stopwords (nltk)\n",
    "stopwords_pt = set(stopwords.words('portuguese'))\n",
    "tokens_sem_stopwords = [token for token in tokens if token not in stopwords_pt]\n",
    "\n",
    "# 4. Stemming (nltk)\n",
    "stemmer = nltk.RSLPStemmer()\n",
    "tokens_stem = [stemmer.stem(token) for token in tokens_sem_stopwords]\n",
    "\n",
    "# 5. Lematiza√ß√£o (spaCy)\n",
    "def lematizar_com_spacy(tokens):\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "    return [token.lemma_ for token in doc]\n",
    "\n",
    "tokens_lematizados = lematizar_com_spacy(tokens_sem_stopwords)\n",
    "\n",
    "# 6. Compara√ß√£o\n",
    "print(\"Texto Original:\\n\", texto)\n",
    "print(\"\\nTexto Normalizado:\\n\", texto_normalizado)\n",
    "print(\"\\nTokens:\\n\", tokens)\n",
    "print(\"\\nTokens Sem Stopwords:\\n\", tokens_sem_stopwords)\n",
    "print(\"\\nStemming:\\n\", tokens_stem)\n",
    "print(\"\\nLematiza√ß√£o:\\n\", tokens_lematizados)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zZSPWzUP4Xd8"
   },
   "source": [
    "### Exemplo 03 - O modelo pre treinado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 805
    },
    "executionInfo": {
     "elapsed": 7998,
     "status": "ok",
     "timestamp": 1742597594200,
     "user": {
      "displayName": "Lucas Martins",
      "userId": "08742951645617950199"
     },
     "user_tz": 180
    },
    "id": "-5MOFbI44YhI",
    "outputId": "53f94517-2c13-4424-c13c-55e76e391520"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Digite um texto para ser analisado: De tudo, ao meu amor serei atento\n",
      "\n",
      "An√°lise gramatical das palavras:\n",
      "Palavra: De, Classe: ADP\n",
      "Palavra: tudo, Classe: PRON\n",
      "Palavra: ,, Classe: PUNCT\n",
      "Palavra: ao, Classe: ADP\n",
      "Palavra: meu, Classe: DET\n",
      "Palavra: amor, Classe: NOUN\n",
      "Palavra: serei, Classe: VERB\n",
      "Palavra: atento, Classe: ADJ\n",
      "\n",
      "Analise de Depend√™ncias:\n",
      "Palavra: De, Depende de: tudo\n",
      "Palavra: tudo, Depende de: serei\n",
      "Palavra: ,, Depende de: tudo\n",
      "Palavra: ao, Depende de: amor\n",
      "Palavra: meu, Depende de: amor\n",
      "Palavra: amor, Depende de: serei\n",
      "Palavra: serei, Depende de: serei\n",
      "Palavra: atento, Depende de: serei\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pt\" id=\"4b62ec84c2764255ab53f339c90dc477-0\" class=\"displacy\" width=\"1275\" height=\"399.5\" direction=\"ltr\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">De</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">tudo,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">ao</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">meu</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">amor</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">serei</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">atento</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4b62ec84c2764255ab53f339c90dc477-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4b62ec84c2764255ab53f339c90dc477-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4b62ec84c2764255ab53f339c90dc477-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,2.0 925.0,2.0 925.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4b62ec84c2764255ab53f339c90dc477-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4b62ec84c2764255ab53f339c90dc477-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4b62ec84c2764255ab53f339c90dc477-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,266.5 L412,254.5 428,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4b62ec84c2764255ab53f339c90dc477-0-3\" stroke-width=\"2px\" d=\"M595,264.5 C595,177.0 740.0,177.0 740.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4b62ec84c2764255ab53f339c90dc477-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,266.5 L587,254.5 603,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4b62ec84c2764255ab53f339c90dc477-0-4\" stroke-width=\"2px\" d=\"M770,264.5 C770,177.0 915.0,177.0 915.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4b62ec84c2764255ab53f339c90dc477-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,266.5 L762,254.5 778,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-4b62ec84c2764255ab53f339c90dc477-0-5\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-4b62ec84c2764255ab53f339c90dc477-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1090.0,266.5 L1098.0,254.5 1082.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Carregar o modelo para portugu√™s\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Processar um texto em portugu√™s\n",
    "textoRecebido = input(\"Digite um texto para ser analisado: \")\n",
    "doc = nlp(textoRecebido)\n",
    "\n",
    "print('\\nAn√°lise gramatical das palavras:')\n",
    "for token in doc:\n",
    "    print(f\"Palavra: {token.text}, Classe: {token.pos_}\")\n",
    "\n",
    "print(\"\\nAnalise de Depend√™ncias:\")\n",
    "for token in doc:\n",
    "  print(f\"Palavra: {token.text}, Depende de: {token.head.text}\")\n",
    "\n",
    "# Visualizar a √°rvore graficamente (opcional)\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_unUIVat4c6u"
   },
   "source": [
    "* O que s√£o: Conjuntos de dados estat√≠sticos e regras, Treinados com milh√µes de textos, Especializados em tarefas espec√≠ficas de linguagem, Resultado de aprendizado de m√°quina\n",
    "* Como s√£o treinados: Alimentados com grande volume de textos, Aprendem padr√µes do idioma, Reconhecem estruturas gramaticais, Identificam rela√ß√µes entre palavras, S√£o testados e refinados\n",
    "* Tipos de modelos por tamanho:\n",
    "  * Pequeno (sm): Mais r√°pido, menor precis√£o, Usa menos mem√≥ria, Bom para testes\n",
    "  * M√©dio (md): Equil√≠brio entre velocidade e precis√£o, Precis√£o moderada\n",
    "Bom para uso geral\n",
    "  * Grande (lg): Mais preciso, Mais lento, Usa mais mem√≥ria, Melhor para an√°lises detalhadas\n",
    "* Alguns modelos pr√©-treinados:\n",
    "  * Portugu√™s - nlp_pt = spacy.load('pt_core_news_sm')\n",
    "  * Ingl√™s - nlp_en = spacy.load('en_core_web_sm')\n",
    "  * Espanhol - nlp_es = spacy.load('es_core_news_sm')\n",
    "  * Franc√™s - nlp_fr = spacy.load('fr_core_news_sm')\n",
    "  * Alem√£o - nlp_de = spacy.load('de_core_news_sm')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM/vPL3GOOc0dnGxOe5agJR",
   "collapsed_sections": [
    "ehZIl9uq38bV",
    "21kKHdWE4CUG",
    "0fipPXw24F_S",
    "SlJ_vLBt4KV-",
    "qd4LujxV4Nyj"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
